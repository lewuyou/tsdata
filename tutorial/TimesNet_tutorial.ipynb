{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimesNet 教程\n",
    "**设置说明：** 这个笔记本提供了由 `TimesNet` 支持的学习任务的教程。\n",
    "\n",
    "`TimesNet` 基本上可以支持 5 个任务，分别是长期预测、短期预测、数据插补、异常检测、分类。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 安装 Python 3.8。为方便起见，执行以下命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft\n",
    "from layers.Embed import DataEmbedding\n",
    "from layers.Conv_Blocks import Inception_Block_V1   \n",
    "            #convolution block used for convoluting the 2D time data, changeable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TimesBlock 构建\n",
    " `TimesNet` 的核心思想在于 `TimesBlock` 的构建。它通常通过对数据实施 FFT（快速傅立叶变换）来获取基本频率，然后分别将时间序列重塑为从主基本频率的 2D 变化，接着进行 2D 卷积，其输出被重塑并加权以形成最终输出。\n",
    "\n",
    " 在以下部分，我们将详细了解 `TimesBlock`。\n",
    "\n",
    " TimesBlock 有两个成员。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimesBlock(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，让我们关注 `__init__(self, configs):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, configs):    ##configs is the configuration defined for TimesBlock\n",
    "    super(TimesBlock, self).__init__() \n",
    "    self.seq_len = configs.seq_len   ##sequence length \n",
    "    self.pred_len = configs.pred_len ##prediction length\n",
    "    self.k = configs.top_k    ##k denotes how many top frequencies are \n",
    "                                                            #taken into consideration\n",
    "    # parameter-efficient design\n",
    "    self.conv = nn.Sequential(\n",
    "        Inception_Block_V1(configs.d_model, configs.d_ff,\n",
    "                           num_kernels=configs.num_kernels),\n",
    "        nn.GELU(),\n",
    "        Inception_Block_V1(configs.d_ff, configs.d_model,\n",
    "                           num_kernels=configs.num_kernels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，来看一下 `forward(self, x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "        B, T, N = x.size()\n",
    "            #B: batch size  T: length of time series  N:number of features\n",
    "        period_list, period_weight = FFT_for_Period(x, self.k)\n",
    "            #FFT_for_Period() will be shown later. Here, period_list([top_k]) denotes \n",
    "            #the top_k-significant period and period_weight([B, top_k]) denotes its weight(amplitude)\n",
    "\n",
    "        res = []\n",
    "        for i in range(self.k):\n",
    "            period = period_list[i]\n",
    "\n",
    "            # padding : to form a 2D map, we need total length of the sequence, plus the part \n",
    "            # to be predicted, to be divisible by the period, so padding is needed\n",
    "            if (self.seq_len + self.pred_len) % period != 0:\n",
    "                length = (\n",
    "                                 ((self.seq_len + self.pred_len) // period) + 1) * period\n",
    "                padding = torch.zeros([x.shape[0], (length - (self.seq_len + self.pred_len)), x.shape[2]]).to(x.device)\n",
    "                out = torch.cat([x, padding], dim=1)\n",
    "            else:\n",
    "                length = (self.seq_len + self.pred_len)\n",
    "                out = x\n",
    "\n",
    "            # reshape: we need each channel of a single piece of data to be a 2D variable,\n",
    "            # Also, in order to implement the 2D conv later on, we need to adjust the 2 dimensions \n",
    "            # to be convolutioned to the last 2 dimensions, by calling the permute() func.\n",
    "            # Whereafter, to make the tensor contiguous in memory, call contiguous()\n",
    "            out = out.reshape(B, length // period, period,\n",
    "                              N).permute(0, 3, 1, 2).contiguous()\n",
    "            \n",
    "            #2D convolution to grap the intra- and inter- period information\n",
    "            out = self.conv(out)\n",
    "\n",
    "            # reshape back, similar to reshape\n",
    "            out = out.permute(0, 2, 3, 1).reshape(B, -1, N)\n",
    "            \n",
    "            #truncating down the padded part of the output and put it to result\n",
    "            res.append(out[:, :(self.seq_len + self.pred_len), :])\n",
    "        res = torch.stack(res, dim=-1) #res: 4D [B, length , N, top_k]\n",
    "\n",
    "        # adaptive aggregation\n",
    "        #First, use softmax to get the normalized weight from amplitudes --> 2D [B,top_k]\n",
    "        period_weight = F.softmax(period_weight, dim=1) \n",
    "\n",
    "        #after two unsqueeze(1),shape -> [B,1,1,top_k],so repeat the weight to fit the shape of res\n",
    "        period_weight = period_weight.unsqueeze(\n",
    "            1).unsqueeze(1).repeat(1, T, N, 1)\n",
    "        \n",
    "        #add by weight the top_k periods' result, getting the result of this TimesBlock\n",
    "        res = torch.sum(res * period_weight, -1)\n",
    "\n",
    "        # residual connection\n",
    "        res = res + x\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面提到的 `FFT_for_Period` 是由以下给出的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT_for_Period(x, k=2):\n",
    "    # xf shape [B, T, C], denoting the amplitude of frequency(T) given the datapiece at B,N\n",
    "    xf = torch.fft.rfft(x, dim=1) \n",
    "\n",
    "    # find period by amplitudes: here we assume that the periodic features are basically constant\n",
    "    # in different batch and channel, so we mean out these two dimensions, getting a list frequency_list with shape[T] \n",
    "    # each element at pos t of frequency_list denotes the overall amplitude at frequency (t)\n",
    "    frequency_list = abs(xf).mean(0).mean(-1) \n",
    "    frequency_list[0] = 0\n",
    "\n",
    "    #by torch.topk(),we can get the biggest k elements of frequency_list, and its positions(i.e. the k-main frequencies in top_list)\n",
    "    _, top_list = torch.topk(frequency_list, k)\n",
    "\n",
    "    #Returns a new Tensor 'top_list', detached from the current graph.\n",
    "    #The result will never require gradient.Convert to a numpy instance\n",
    "    top_list = top_list.detach().cpu().numpy()\n",
    "     \n",
    "    #period:a list of shape [top_k], recording the periods of mean frequencies respectively\n",
    "    period = x.shape[1] // top_list\n",
    "\n",
    "    #Here,the 2nd item returned has a shape of [B, top_k],representing the biggest top_k amplitudes \n",
    "    # for each piece of data, with N features being averaged.\n",
    "    return period, abs(xf).mean(-1)[:, top_list] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了更清楚地说明，请参见下面的图表。\n",
    "\n",
    "![FFT demonstrator](./fft.png)\n",
    "\n",
    "![2D Conv demonstrator](./conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欲了解更多详情，请阅读我们的论文\n",
    "(link: https://openreview.net/pdf?id=ju_Uqw384Oq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TimesNet\n",
    "\n",
    "到目前为止，我们已经得到了擅长检索内部和跨期时间信息的 `TimesBlock`。我们现在可以构建一个 `TimesNet`。`TimesNet` 擅长多任务处理，包括短期和长期预测、数据插补、分类以及异常检测。\n",
    "\n",
    "在这一部分，我们将详细概述 `TimesNet` 如何在这些任务中发挥其能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        ...\n",
    "    \n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        ...\n",
    "\n",
    "    def imputation(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask):\n",
    "        ...\n",
    "\n",
    "    def anomaly_detection(self, x_enc):\n",
    "        ...\n",
    "    \n",
    "    def classification(self, x_enc, x_mark_enc):\n",
    "        ...\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，让我们关注 `__init__(self, configs):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, configs):\n",
    "    super(Model, self).__init__()\n",
    "    #params init\n",
    "    self.configs = configs\n",
    "    self.task_name = configs.task_name\n",
    "    self.seq_len = configs.seq_len\n",
    "    self.label_len = configs.label_len\n",
    "    self.pred_len = configs.pred_len\n",
    "\n",
    "    #stack TimesBlock for e_layers times to form the main part of TimesNet, named model\n",
    "    self.model = nn.ModuleList([TimesBlock(configs)\n",
    "                                for _ in range(configs.e_layers)])\n",
    "    \n",
    "    #embedding & normalization\n",
    "    # enc_in is the encoder input size, the number of features for a piece of data\n",
    "    # d_model is the dimension of embedding\n",
    "    self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                       configs.dropout)\n",
    "    self.layer = configs.e_layers # num of encoder layers\n",
    "    self.layer_norm = nn.LayerNorm(configs.d_model)\n",
    "\n",
    "    #define the some layers for different tasks\n",
    "    if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "        self.predict_linear = nn.Linear(\n",
    "            self.seq_len, self.pred_len + self.seq_len)\n",
    "        self.projection = nn.Linear(\n",
    "            configs.d_model, configs.c_out, bias=True)\n",
    "    if self.task_name == 'imputation' or self.task_name == 'anomaly_detection':\n",
    "        self.projection = nn.Linear(\n",
    "            configs.d_model, configs.c_out, bias=True)\n",
    "    if self.task_name == 'classification':\n",
    "        self.act = F.gelu\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "        self.projection = nn.Linear(\n",
    "            configs.d_model * configs.seq_len, configs.num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 预测\n",
    "\n",
    "预测的基本思想是将已知序列长度延长到 (seq_len+pred_len)，这是预测后的总长度。然后通过几层 TimesBlock 层以及层归一化，表示一些潜在的内部和跨期信息。有了这些信息，我们可以将其投射到输出空间。之后通过去归一化（如果是非平稳变换器），我们得到最终输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "    # Normalization from Non-stationary Transformer at temporal dimension\n",
    "    means = x_enc.mean(1, keepdim=True).detach() #[B,T]\n",
    "    x_enc = x_enc - means\n",
    "    stdev = torch.sqrt(\n",
    "        torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "    x_enc /= stdev\n",
    "\n",
    "    # embedding: projecting a number to a C-channel vector\n",
    "    enc_out = self.enc_embedding(x_enc, x_mark_enc)  # [B,T,C] C is d_model\n",
    "    enc_out = self.predict_linear(enc_out.permute(0, 2, 1)).permute(\n",
    "        0, 2, 1)  # align temporal dimension [B,pred_len+seq_len,C]\n",
    "    \n",
    "    # TimesNet: pass through TimesBlock for self.layer times each with layer normalization\n",
    "    for i in range(self.layer):\n",
    "        enc_out = self.layer_norm(self.model[i](enc_out))\n",
    "\n",
    "    # project back  #[B,T,d_model]-->[B,T,c_out]\n",
    "    dec_out = self.projection(enc_out) \n",
    "\n",
    "    # De-Normalization from Non-stationary Transformer\n",
    "    dec_out = dec_out * \\\n",
    "              (stdev[:, 0, :].unsqueeze(1).repeat(\n",
    "                  1, self.pred_len + self.seq_len, 1)) #lengthen the stdev to fit the dec_out\n",
    "    dec_out = dec_out + \\\n",
    "              (means[:, 0, :].unsqueeze(1).repeat(\n",
    "                  1, self.pred_len + self.seq_len, 1)) #lengthen the mean to fit the dec_out\n",
    "    return dec_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 数据插补\n",
    "\n",
    "数据插补是一项旨在完成时间序列中某些缺失值的任务，因此在某种程度上它与预测类似。我们仍然可以使用类似的步骤来处理它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask):\n",
    "    # Normalization from Non-stationary Transformer\n",
    "    means = torch.sum(x_enc, dim=1) / torch.sum(mask == 1, dim=1)\n",
    "    means = means.unsqueeze(1).detach()\n",
    "    x_enc = x_enc - means\n",
    "    x_enc = x_enc.masked_fill(mask == 0, 0)\n",
    "    stdev = torch.sqrt(torch.sum(x_enc * x_enc, dim=1) /\n",
    "                       torch.sum(mask == 1, dim=1) + 1e-5)\n",
    "    stdev = stdev.unsqueeze(1).detach()\n",
    "    x_enc /= stdev\n",
    "\n",
    "    # embedding\n",
    "    enc_out = self.enc_embedding(x_enc, x_mark_enc)  # [B,T,C]\n",
    "    # TimesNet\n",
    "    for i in range(self.layer):\n",
    "        enc_out = self.layer_norm(self.model[i](enc_out))\n",
    "    # project back\n",
    "    dec_out = self.projection(enc_out)\n",
    "\n",
    "    # De-Normalization from Non-stationary Transformer\n",
    "    dec_out = dec_out * \\\n",
    "              (stdev[:, 0, :].unsqueeze(1).repeat(\n",
    "                  1, self.pred_len + self.seq_len, 1))\n",
    "    dec_out = dec_out + \\\n",
    "              (means[:, 0, :].unsqueeze(1).repeat(\n",
    "                  1, self.pred_len + self.seq_len, 1))\n",
    "    return dec_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 异常检测\n",
    "\n",
    "与数据插补类似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_detection(self, x_enc):\n",
    "    # Normalization from Non-stationary Transformer\n",
    "    means = x_enc.mean(1, keepdim=True).detach()\n",
    "    x_enc = x_enc - means\n",
    "    stdev = torch.sqrt(\n",
    "        torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "    x_enc /= stdev\n",
    "    # embedding\n",
    "    enc_out = self.enc_embedding(x_enc, None)  # [B,T,C]\n",
    "    # TimesNet\n",
    "    for i in range(self.layer):\n",
    "        enc_out = self.layer_norm(self.model[i](enc_out))\n",
    "    # project back\n",
    "    dec_out = self.projection(enc_out)\n",
    "    # De-Normalization from Non-stationary Transformer\n",
    "    dec_out = dec_out * \\\n",
    "              (stdev[:, 0, :].unsqueeze(1).repeat(\n",
    "                  1, self.pred_len + self.seq_len, 1))\n",
    "    dec_out = dec_out + \\\n",
    "              (means[:, 0, :].unsqueeze(1).repeat(\n",
    "                  1, self.pred_len + self.seq_len, 1))\n",
    "    return dec_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(self, x_enc, x_mark_enc):\n",
    "        # embedding\n",
    "        enc_out = self.enc_embedding(x_enc, None)  # [B,T,C]\n",
    "        # TimesNet\n",
    "        for i in range(self.layer):\n",
    "            enc_out = self.layer_norm(self.model[i](enc_out))\n",
    "\n",
    "        # Output\n",
    "        # the output transformer encoder/decoder embeddings don't include non-linearity\n",
    "        output = self.act(enc_out)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # zero-out padding embeddings:The primary role of x_mark_enc in the code is to \n",
    "        # zero out the embeddings for padding positions in the output tensor through \n",
    "        # element-wise multiplication, helping the model to focus on meaningful data \n",
    "        # while disregarding padding.\n",
    "        output = output * x_mark_enc.unsqueeze(-1)\n",
    "        \n",
    "        # (batch_size, seq_length * d_model)\n",
    "        output = output.reshape(output.shape[0], -1)\n",
    "        output = self.projection(output)  # (batch_size, num_classes)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，通过上述这么多任务，我们能够完成 `forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "    if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "        dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "        return dec_out[:, -self.pred_len:, :]  # [B, L, D] return the predicted part of sequence\n",
    "    if self.task_name == 'imputation':\n",
    "        dec_out = self.imputation(\n",
    "            x_enc, x_mark_enc, x_dec, x_mark_dec, mask)\n",
    "        return dec_out  # [B, L, D] return the whole sequence with missing value estimated\n",
    "    if self.task_name == 'anomaly_detection':\n",
    "        dec_out = self.anomaly_detection(x_enc)\n",
    "        return dec_out  # [B, L, D] return the sequence that should be correct\n",
    "    if self.task_name == 'classification':\n",
    "        dec_out = self.classification(x_enc, x_mark_enc)\n",
    "        return dec_out  # [B, N] return the classification result\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 训练和设置\n",
    "\n",
    "到目前为止，我们已经成功构建了 `TimesNet`。现在我们面临的问题是如何训练和测试这个神经网络。在 __*exp*__ 部分实现了训练、验证以及测试的操作，在这里汇集了不同任务的代码。这些实验不仅适用于 `TimesNet` 的训练，也适用于任何其他时间序列表示模型的训练。但在这里，我们仅使用 `TimesNet` 进行分析。\n",
    "\n",
    "`TimesNet` 在多个任务中都是最先进的，但在这里我们只介绍其长期预测任务的训练，因为其他任务的训练过程的主干与此类似。再次强调，一旦你了解了训练过程的工作原理，测试和验证代码就很容易理解了。因此，首先我们将重点关注 `TimesNet` 在长期预测任务上的训练。\n",
    "\n",
    "我们将讨论许多方面，包括训练过程、训练损失等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 长期预测任务的训练\n",
    "\n",
    "以下代码代表了为长期预测任务训练模型的过程。我们将详细查看它。简要来说，训练部分可以大致分为几个部分，包括数据准备、创建保存路径、初始化、优化器和损失函数选择、使用混合精度训练、训练循环、验证和提前停止、学习率调整、加载最佳模型。\n",
    "\n",
    "更多细节，请参见下面的代码。'train' 过程定义在实验 <font color=orange>__class Exp_Long_Term_Forecast__</font> 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, setting):  #setting is the args for this model training\n",
    "    #get train dataloader\n",
    "    train_data, train_loader = self._get_data(flag='train')\n",
    "    vali_data, vali_loader = self._get_data(flag='val')\n",
    "    test_data, test_loader = self._get_data(flag='test')\n",
    "\n",
    "    # set path of checkpoint for saving and loading model\n",
    "    path = os.path.join(self.args.checkpoints, setting)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    time_now = time.time()\n",
    "\n",
    "    train_steps = len(train_loader)\n",
    "\n",
    "    # EarlyStopping is typically a custom class or function that monitors the performance \n",
    "    # of a model during training, usually by tracking a certain metric (commonly validation \n",
    "    # loss or accuracy).It's a common technique used in deep learning to prevent overfitting \n",
    "    # during the training\n",
    "    early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)\n",
    "\n",
    "    #Optimizer and Loss Function Selection\n",
    "    model_optim = self._select_optimizer()\n",
    "    criterion = self._select_criterion()\n",
    "\n",
    "    # AMP training is a technique that uses lower-precision data types (e.g., float16) \n",
    "    # for certain computations to accelerate training and reduce memory usage.\n",
    "    if self.args.use_amp:  \n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "    for epoch in range(self.args.train_epochs):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "        self.model.train()\n",
    "        epoch_time = time.time()\n",
    "\n",
    "        #begin training in this epoch\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "            batch_x = batch_x.float().to(self.device)  #input features\n",
    "            batch_y = batch_y.float().to(self.device)  #target features\n",
    "\n",
    "            # _mark holds information about time-related features. Specifically, it is a \n",
    "            # tensor that encodes temporal information and is associated with the \n",
    "            # input data batch_x.\n",
    "            batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "            batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "            # decoder input(didn't use in TimesNet case)\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "            # encoder - decoder\n",
    "            if self.args.use_amp: #in the case of TimesNet, use_amp should be False\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # whether to output attention in ecoder,in TimesNet case is no\n",
    "                    if self.args.output_attention: \n",
    "                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                    # model the input\n",
    "                    else:\n",
    "                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "                    # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, \n",
    "                    # S:univariate predict univariate, MS:multivariate predict univariate'\n",
    "                    #if multivariate predict univariate',then output should be the last column of the decoder\n",
    "                    # output, so f_dim = -1 to only contain the last column, else is all columns\n",
    "                    f_dim = -1 if self.args.features == 'MS' else 0 \n",
    "                    outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                    batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
    "\n",
    "                    # calc loss\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    train_loss.append(loss.item())\n",
    "            else:  #similar to when use_amp is True\n",
    "                if self.args.output_attention:\n",
    "                    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                else:\n",
    "                    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            # When train rounds attain some 100-multiple, print speed, left time, loss. etc feedback\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                speed = (time.time() - time_now) / iter_count\n",
    "                left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n",
    "                print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                iter_count = 0\n",
    "                time_now = time.time()\n",
    "\n",
    "            #BP\n",
    "            if self.args.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(model_optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                model_optim.step()\n",
    "        \n",
    "        #This epoch comes to end, print information\n",
    "        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "\n",
    "        #run test and validation on current model\n",
    "        vali_loss = self.vali(vali_data, vali_loader, criterion)\n",
    "        test_loss = self.vali(test_data, test_loader, criterion)\n",
    "\n",
    "        #print train, test, vali loss information\n",
    "        print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "            epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "        \n",
    "        #Decide whether to trigger Early Stopping. if early_stop is true, it means that \n",
    "        #this epoch's training is now at a flat slope, so stop further training for this epoch.\n",
    "        early_stopping(vali_loss, self.model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        #adjust learning keys\n",
    "        adjust_learning_rate(model_optim, epoch + 1, self.args)\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "\n",
    "    # loading the trained model's state dictionary from a saved checkpoint file \n",
    "    # located at best_model_path.\n",
    "    self.model.load_state_dict(torch.load(best_model_path))\n",
    "    return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您想了解更多，请查看 exp/exp_long_term_forecasting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 提前停止机制\n",
    "\n",
    "<font color=purple>__EarlyStopping__</font> 通常是一个自定义类或函数，用于在训练期间监控模型的性能，通常通过跟踪某个指标（通常是验证损失或准确度）来实现。这是深度学习中常用的技术，用于防止训练过程中的过拟合。\n",
    "\n",
    "让我们看下面的代码（原始代码在 `tools.py` 中）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience # how many times will you tolerate for loss not being on decrease\n",
    "        self.verbose = verbose  # whether to print tip info\n",
    "        self.counter = 0 # now how many times loss not on decrease\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "\n",
    "        # meaning: current score is not 'delta' better than best_score, representing that \n",
    "        # further training may not bring remarkable improvement in loss. \n",
    "        elif score < self.best_score + self.delta:  \n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            # 'No Improvement' times become higher than patience --> Stop Further Training\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "        else: #model's loss is still on decrease, save the now best model and go on training\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "    ### used for saving the current best model\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 优化器和准则\n",
    "\n",
    "优化器和准则在 <font color=orange>__class Exp_Long_Term_Forecast__</font> 中定义，并通过函数 `self._select_optimizer()` 和 `self._select_criterion()` 在训练过程中调用。在这里，对于长期预测任务，我们简单地采用 Adam 优化器和 MSELoss 来衡量真实数据与预测数据之间的损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_optimizer(self):\n",
    "    model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n",
    "    return model_optim\n",
    "\n",
    "def _select_criterion(self):\n",
    "    criterion = nn.MSELoss()\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 自动混合精度（AMP）\n",
    "\n",
    "AMP 是深度学习中用来提高训练速度和减少内存使用的技术。AMP 通过混合半精度（16位浮点数）和单精度（32位浮点数）计算来实现这一点。\n",
    "\n",
    "让我们更仔细地看看这个代码片段："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in forward process:\n",
    "with torch.cuda.amp.autocast():\n",
    "\n",
    "...\n",
    "\n",
    "#in BP process:\n",
    "if self.args.use_amp:\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(model_optim)\n",
    "    scaler.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`with torch.cuda.amp.autocast()`: 使用 `torch.cuda.amp.autocast()` 的目的是利用混合精度训练的速度和内存效率优势，同时保持数值稳定性。某些深度学习模型可以从这项技术中获得显著的好处，尤其是在支持半精度算术的现代 GPU 上。它允许你更快地执行某些计算，同时仍然确保关键计算（例如梯度更新）以足够的精度执行，以避免准确性的损失。\n",
    "\n",
    "`scaler.scale(loss).backward()`: 如果启用了 AMP，它使用通过 `torch.cuda.amp.GradScaler()` 创建的 scaler 对象自动缩放损失并执行反向传播。这是 AMP 的关键部分，确保数值稳定性。在反向传播之前，损失被缩放到适当的范围，以防止梯度过快发散或造成数值不稳定。\n",
    "\n",
    "`scaler.step(model_optim)`: 接下来，scaler 调用 step 方法，该方法将缩放后的梯度应用于模型的优化器（model_optim）。这用于更新模型的权重，以最小化损失函数。\n",
    "\n",
    "`scaler.update()`: 最后，scaler 调用 update 方法，更新缩放因子以确保下一次迭代的损失正确缩放。这一步有助于动态调整梯度的缩放，以适应不同的训练场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 学习率调整\n",
    "\n",
    "虽然优化器负责随着时代的变化调整学习率，但我们仍然希望手动对其进行一些调整，如函数 `adjust_learning_rate(model_optim, epoch + 1, self.args)` 所示，其代码如下所示（原始代码在 `tools.py` 中）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "\n",
    "    #first type: learning rate decrease with epoch by exponential\n",
    "    if args.lradj == 'type1':\n",
    "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "\n",
    "    #second type: learning rate decrease manually\n",
    "    elif args.lradj == 'type2':\n",
    "        lr_adjust = {\n",
    "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "        }\n",
    "\n",
    "    #1st type: update in each epoch\n",
    "    #2nd type: only update in epochs that are written in Dict lr_adjust\n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "    \n",
    "        # change the learning rate for different parameter groups within the optimizer\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        print('Updating learning rate to {}'.format(lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 验证和测试\n",
    "\n",
    "在训练过程中，模型不断调整其权重和参数以最小化训练误差。然而，这可能并不反映模型对未见数据的性能。验证允许我们定期评估模型在与训练数据不同的数据上的性能，提供对模型泛化能力的见解。\n",
    "\n",
    "通过比较验证集上的性能，我们可以识别模型是否过拟合。过拟合是指模型在训练数据上表现良好，但在未见数据上表现不佳的情况。监控验证集上的性能有助于及早发现过拟合，并采取措施防止它，如提前停止或调整超参数。\n",
    "\n",
    "在这里，我们仍然以长期预测为例，类似于训练过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vali(self, vali_data, vali_loader, criterion):\n",
    "        total_loss = []\n",
    "\n",
    "        #evaluation mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                batch_y = batch_y.float()\n",
    "\n",
    "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "                # encoder - decoder\n",
    "                if self.args.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if self.args.output_attention:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if self.args.output_attention:\n",
    "                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                    else:\n",
    "                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
    "\n",
    "                pred = outputs.detach().cpu()\n",
    "                true = batch_y.detach().cpu()\n",
    "\n",
    "                loss = criterion(pred, true)\n",
    "\n",
    "                total_loss.append(loss)\n",
    "        total_loss = np.average(total_loss)\n",
    "        self.model.train()\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试与验证类似，但其目的是检查模型的表现如何，因此通常会添加一些使用 __matplotlib.pyplot__ 的可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visual(true, preds=None, name='./pic/test.pdf'):\n",
    "    \"\"\"\n",
    "    Results visualization\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(true, label='GroundTruth', linewidth=2)\n",
    "    if preds is not None:\n",
    "        plt.plot(preds, label='Prediction', linewidth=2)\n",
    "    plt.legend()\n",
    "    plt.savefig(name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self, setting, test=0):\n",
    "        test_data, test_loader = self._get_data(flag='test')\n",
    "        if test:\n",
    "            print('loading model')\n",
    "            self.model.load_state_dict(torch.load(os.path.join('./checkpoints/' + setting, 'checkpoint.pth')))\n",
    "\n",
    "        preds = []\n",
    "        trues = []\n",
    "        folder_path = './test_results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                batch_y = batch_y.float().to(self.device)\n",
    "\n",
    "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "                # encoder - decoder\n",
    "                if self.args.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if self.args.output_attention:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if self.args.output_attention:\n",
    "                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "\n",
    "                    else:\n",
    "                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "                f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                batch_y = batch_y.detach().cpu().numpy()\n",
    "\n",
    "                #inverse the data if scaled\n",
    "                if test_data.scale and self.args.inverse:\n",
    "                    outputs = test_data.inverse_transform(outputs)\n",
    "                    batch_y = test_data.inverse_transform(batch_y)\n",
    "\n",
    "                pred = outputs\n",
    "                true = batch_y\n",
    "\n",
    "                preds.append(pred)\n",
    "                trues.append(true)\n",
    "\n",
    "                #visualize one piece of data every 20\n",
    "                if i % 20 == 0:\n",
    "                    input = batch_x.detach().cpu().numpy()\n",
    "                    #the whole sequence\n",
    "                    gt = np.concatenate((input[0, :, -1], true[0, :, -1]), axis=0)\n",
    "                    pd = np.concatenate((input[0, :, -1], pred[0, :, -1]), axis=0)\n",
    "                    visual(gt, pd, os.path.join(folder_path, str(i) + '.pdf'))\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        trues = np.array(trues)  # shape[batch_num, batch_size, pred_len, features]\n",
    "        print('test shape:', preds.shape, trues.shape)\n",
    "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
    "        trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n",
    "        print('test shape:', preds.shape, trues.shape)\n",
    "\n",
    "        # result save\n",
    "        folder_path = './results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        mae, mse, rmse, mape, mspe = metric(preds, trues)\n",
    "        print('mse:{}, mae:{}'.format(mse, mae))\n",
    "        f = open(\"result_long_term_forecast.txt\", 'a')\n",
    "        f.write(setting + \"  \\n\")\n",
    "        f.write('mse:{}, mae:{}'.format(mse, mae))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "        \n",
    "        np.save(folder_path + 'metrics.npy', np.array([mae, mse, rmse, mape, mspe]))\n",
    "        np.save(folder_path + 'pred.npy', preds)\n",
    "        np.save(folder_path + 'true.npy', trues)\n",
    "\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 数据加载器和数据提供者\n",
    "\n",
    "在训练过程中，我们简单地默认使用数据加载器，通过函数 `self._get_data(flag='train')`。那么这行代码是如何工作的？让我们看看定义（在 <font color=orange>__class Exp_Long_Term_Forecast__</font> 中）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_data(self, flag):\n",
    "        data_set, data_loader = data_provider(self.args, flag)\n",
    "        return data_set, data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再进一步，看看 `data_provider(self.args, flag)`（在 `data_factory.py` 中）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are some dataloaders defined in data_loader.py. If you want to add your own data, \n",
    "# go and check data_loader.py to rewrite a dataloader to fit your data.\n",
    "data_dict = {\n",
    "    'ETTh1': Dataset_ETT_hour,\n",
    "    'ETTh2': Dataset_ETT_hour,\n",
    "    'ETTm1': Dataset_ETT_minute,\n",
    "    'ETTm2': Dataset_ETT_minute,\n",
    "    'custom': Dataset_Custom,\n",
    "    'm4': Dataset_M4,\n",
    "    'PSM': PSMSegLoader,\n",
    "    'MSL': MSLSegLoader,\n",
    "    'SMAP': SMAPSegLoader,\n",
    "    'SMD': SMDSegLoader,\n",
    "    'SWAT': SWATSegLoader,\n",
    "    'UEA': UEAloader\n",
    "}\n",
    "\n",
    "\n",
    "def data_provider(args, flag):\n",
    "    Data = data_dict[args.data]  #data_provider\n",
    "\n",
    "    # time features encoding, options:[timeF, fixed, learned]\n",
    "    timeenc = 0 if args.embed != 'timeF' else 1\n",
    "\n",
    "    #test data provider\n",
    "    if flag == 'test':\n",
    "        shuffle_flag = False\n",
    "        drop_last = True\n",
    "        if args.task_name == 'anomaly_detection' or args.task_name == 'classification':\n",
    "            batch_size = args.batch_size\n",
    "\n",
    "        #Some tasks during the testing phase may require evaluating samples one at a time. \n",
    "        # This could be due to variations in sample sizes in the test data or because the \n",
    "        # evaluation process demands finer-grained results or different processing. \n",
    "        else:\n",
    "            batch_size = 1  # bsz=1 for evaluation\n",
    "\n",
    "        #freq for time features encoding, \n",
    "        # options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly,\n",
    "        #  m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "        freq = args.freq\n",
    "    else:\n",
    "        shuffle_flag = True\n",
    "        drop_last = True\n",
    "        batch_size = args.batch_size  # bsz for train and valid\n",
    "        freq = args.freq\n",
    "\n",
    "    if args.task_name == 'anomaly_detection':\n",
    "        drop_last = False\n",
    "        data_set = Data(\n",
    "            root_path=args.root_path, #root path of the data file\n",
    "            win_size=args.seq_len,    #input sequence length\n",
    "            flag=flag,\n",
    "        )\n",
    "        print(flag, len(data_set))\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=args.num_workers,#data loader num workers\n",
    "            drop_last=drop_last)\n",
    "        return data_set, data_loader\n",
    "\n",
    "    elif args.task_name == 'classification':\n",
    "        drop_last = False\n",
    "        data_set = Data(\n",
    "            root_path=args.root_path,\n",
    "            flag=flag,\n",
    "        )\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=drop_last,\n",
    "            collate_fn=lambda x: collate_fn(x, max_len=args.seq_len) \n",
    "            #define some limits to collate pieces of data into batches\n",
    "        )\n",
    "        return data_set, data_loader\n",
    "    else:\n",
    "        if args.data == 'm4':\n",
    "            drop_last = False\n",
    "        data_set = Data(\n",
    "            root_path=args.root_path, #eg.  ./data/ETT/\n",
    "            data_path=args.data_path, #eg.  ETTh1.csv\n",
    "            flag=flag,\n",
    "            size=[args.seq_len, args.label_len, args.pred_len],\n",
    "            features=args.features,   #forecasting task, options:[M, S, MS]; \n",
    "            # M:multivariate predict multivariate, S:univariate predict univariate,\n",
    "            # MS:multivariate predict univariate\n",
    "            \n",
    "            target=args.target,       #target feature in S or MS task\n",
    "            timeenc=timeenc,\n",
    "            freq=freq,\n",
    "            seasonal_patterns=args.seasonal_patterns\n",
    "        )\n",
    "        print(flag, len(data_set))\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=drop_last)\n",
    "        return data_set, data_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的描述中，我们可以轻松地发现，data_provider 负责根据不同的任务和运行模式将数据集整理成批次。它将参数传递给数据加载器（`Data`）以指导其如何将一个数据文件管理成可用的数据片段。然后，它还通过将构建好的数据集和一些其他参数传递给标准类 Dataloader 来生成最终的 dara_loader。之后，就生成了适合模型需求的数据集和一个可枚举的数据加载器。\n",
    "\n",
    "那么如何组织数据文件，使之成为适合模型的数据片段呢？让我们看看 `data_loader.py`！其中有许多数据加载器，当然你也可以编写自己的数据加载器，但在这里我们只关注 <font color=orange>__class Dataset_ETT_hour(Dataset)__</font> 作为示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_ETT_hour(Dataset):\n",
    "        def __init__(self, root_path, flag='train', size=None,\n",
    "                 features='S', data_path='ETTh1.csv',\n",
    "                 target='OT', scale=True, timeenc=0, freq='h', seasonal_patterns=None):\n",
    "                ...     \n",
    "        def __read_data__(self):\n",
    "                ...     \n",
    "        def __getitem__(self, index):\n",
    "                ...\n",
    "        \n",
    "        def __len__(self):\n",
    "                ...\n",
    "        \n",
    "        def inverse_transform(self, data):\n",
    "                ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__init__()` 是用于初始化数据集的各种参数和属性的构造函数。它接受一系列参数，包括数据文件的路径、数据集的标志（例如，训练、验证、测试）、数据集大小、特征类型、目标变量、是否缩放数据、时间编码、时间频率等。这些参数用于配置数据集的加载和处理方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, root_path, flag='train', size=None,\n",
    "             features='S', data_path='ETTh1.csv',\n",
    "             target='OT', scale=True, timeenc=0, freq='h', seasonal_patterns=None):\n",
    "    # size [seq_len, label_len, pred_len]\n",
    "    # info\n",
    "    if size == None:\n",
    "        self.seq_len = 24 * 4 * 4\n",
    "        self.label_len = 24 * 4\n",
    "        self.pred_len = 24 * 4\n",
    "    else:\n",
    "        self.seq_len = size[0]\n",
    "        self.label_len = size[1]\n",
    "        self.pred_len = size[2]\n",
    "    # init\n",
    "    assert flag in ['train', 'test', 'val']\n",
    "    type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "    self.set_type = type_map[flag]\n",
    "    self.features = features\n",
    "    self.target = target\n",
    "    self.scale = scale\n",
    "    self.timeenc = timeenc\n",
    "    self.freq = freq\n",
    "    self.root_path = root_path\n",
    "    self.data_path = data_path\n",
    "    \n",
    "    # After initialization, call __read_data__() to manage the data file.\n",
    "    self.__read_data__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据文件管理成可用数据片段的实际过程发生在 `__read_data__()` 中，见下文："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __read_data__(self):\n",
    "    self.scaler = StandardScaler()\n",
    "\n",
    "    #get raw data from path\n",
    "    df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                      self.data_path))\n",
    "\n",
    "    # split data set into train, vali, test. border1 is the left border and border2 is the right.\n",
    "    # Once flag(train, vali, test) is determined, __read_data__ will return certain part of the dataset.\n",
    "    border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n",
    "    border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n",
    "    border1 = border1s[self.set_type]\n",
    "    border2 = border2s[self.set_type]\n",
    "\n",
    "    #decide which columns to select\n",
    "    if self.features == 'M' or self.features == 'MS':\n",
    "        cols_data = df_raw.columns[1:] # column name list (remove 'date')\n",
    "        df_data = df_raw[cols_data]  #remove the first column, which is time stamp info\n",
    "    elif self.features == 'S':\n",
    "        df_data = df_raw[[self.target]] # target column\n",
    "\n",
    "    #scale data by the scaler that fits training data\n",
    "    if self.scale:\n",
    "        train_data = df_data[border1s[0]:border2s[0]]\n",
    "        #train_data.values: turn pandas DataFrame into 2D numpy\n",
    "        self.scaler.fit(train_data.values)  \n",
    "        data = self.scaler.transform(df_data.values)\n",
    "    else:\n",
    "        data = df_data.values \n",
    "    \n",
    "    #time stamp:df_stamp is a object of <class 'pandas.core.frame.DataFrame'> and\n",
    "    # has one column called 'date' like 2016-07-01 00:00:00\n",
    "    df_stamp = df_raw[['date']][border1:border2]\n",
    "    \n",
    "    # Since the date format is uncertain across different data file, we need to \n",
    "    # standardize it so we call func 'pd.to_datetime'\n",
    "    df_stamp['date'] = pd.to_datetime(df_stamp.date) \n",
    "\n",
    "    if self.timeenc == 0:  #time feature encoding is fixed or learned\n",
    "        df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "        df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "        df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "        df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "        #now df_frame has multiple columns recording the month, day etc. time stamp\n",
    "        # next we delete the 'date' column and turn 'DataFrame' to a list\n",
    "        data_stamp = df_stamp.drop(['date'], 1).values\n",
    "\n",
    "    elif self.timeenc == 1: #time feature encoding is timeF\n",
    "        '''\n",
    "         when entering this branch, we choose arg.embed as timeF meaning we want to \n",
    "         encode the temporal info. 'freq' should be the smallest time step, and has \n",
    "          options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "         So you should check the timestep of your data and set 'freq' arg. \n",
    "         After the time_features encoding, each date info format will be encoded into \n",
    "         a list, with each element denoting the relative position of this time point\n",
    "         (e.g. Day of Week, Day of Month, Hour of Day) and each normalized within scope[-0.5, 0.5]\n",
    "         '''\n",
    "        data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "        data_stamp = data_stamp.transpose(1, 0)\n",
    "        \n",
    "    \n",
    "    # data_x and data_y are same copy of a certain part of data\n",
    "    self.data_x = data[border1:border2]\n",
    "    self.data_y = data[border1:border2]\n",
    "    self.data_stamp = data_stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__read_data__()` 将数据集分为三个部分，选择所需的列并管理时间戳信息。它输出了为以后使用而妥善管理的数据数组。接下来，我们必须完成 <font color=orange>__class Dataset__</font> 的重载，看看 `__getitem__(self, index)` 和 `__len__(self)`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, index):\n",
    "    #given an index, calculate the positions after this index to truncate the dataset\n",
    "    s_begin = index\n",
    "    s_end = s_begin + self.seq_len\n",
    "    r_begin = s_end - self.label_len\n",
    "    r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "    #input and output sequence\n",
    "    seq_x = self.data_x[s_begin:s_end]\n",
    "    seq_y = self.data_y[r_begin:r_end]\n",
    "\n",
    "    #time mark\n",
    "    seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "    seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "    return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "def __len__(self):\n",
    "    return len(self.data_x) - self.seq_len - self.pred_len + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要，您还可以为缩放器添加一个 inverse_transform。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(self, data):\n",
    "    return self.scaler.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到目前为止，我们已经完成了数据集和数据加载器的构建。如果您想构建自己的数据并在网络上运行，可以找到合适的数据，并尝试完成上述列出的功能。这里有一些在时间序列分析中广泛使用的数据集。\n",
    "\n",
    "![common dataset](./dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 运行实验和可视化结果\n",
    "\n",
    "在妥善处理数据和模型之后，我们需要为实验编写一个 shell 脚本。在脚本中，我们需要运行 `run.py` 并传递几个参数，这是配置的一部分。在这里，让我们以 `TimesNet` 在长期预测任务上使用 ETTh1 数据集为例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "model_name=TimesNet\n",
    "\n",
    "\n",
    "python -u run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./dataset/ETT-small/ \\\n",
    "  --data_path ETTh1.csv \\\n",
    "  --model_id ETTh1_96_96 \\\n",
    "  --model $model_name \\\n",
    "  --data ETTh1 \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --d_model 16 \\\n",
    "  --d_ff 32 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --top_k 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成 shell 脚本后，您可以在 shell 中使用 bash 运行它。例如，您可以运行以下命令，对于 `TimesNet` ETTh1 长期预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "bash ./scripts/long_term_forecast/ETT_script/TimesNet_ETTh1.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，由于环境中缺乏适当的包，bash 命令可能无法成功执行。如果是这种情况，只需按照错误信息提示逐步安装缺失的包，直到成功为止。实验运行成功的标志是打印出有关实验的信息，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_96_96', model='TimesNet', data='ETTh1', root_path='./dataset/ETT-small/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, seasonal_patterns='Monthly', inverse=False, mask_rate=0.25, anomaly_ratio=0.25, top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='MSE', lradj='type1', use_amp=False, use_gpu=False, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)\n",
    "Use GPU: cuda:0\n",
    ">>>>>>>start training : long_term_forecast_ETTh1_96_96_TimesNet_ETTh1_ftM_sl96_ll48_pl96_dm16_nh8_el2_dl1_df32_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "train 8449\n",
    "val 2785\n",
    "test 2785"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，模型开始训练。一旦一个训练周期完成，将会打印出类似下面的信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "        iters: 100, epoch: 1 | loss: 0.4701951\n",
    "        speed: 0.2108s/iter; left time: 535.7317s\n",
    "        iters: 200, epoch: 1 | loss: 0.4496171\n",
    "        speed: 0.0615s/iter; left time: 150.0223s\n",
    "Epoch: 1 cost time: 30.09317970275879\n",
    "Epoch: 1, Steps: 264 | Train Loss: 0.4964185 Vali Loss: 0.8412074 Test Loss: 0.4290483\n",
    "Validation loss decreased (inf --> 0.841207).  Saving model ...\n",
    "Updating learning rate to 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当所有训练周期都完成后，模型进入测试阶段。关于测试的以下信息将被打印出来，给出了测试的 MAE 和 MSE。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    ">>>>>>>testing : long_term_forecast_ETTh1_96_96_TimesNet_ETTh1_ftM_sl96_ll48_pl96_dm16_nh8_el2_dl1_df32_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "test 2785\n",
    "test shape: (2785, 1, 96, 7) (2785, 1, 96, 7)\n",
    "test shape: (2785, 96, 7) (2785, 96, 7)\n",
    "mse:0.3890332877635956, mae:0.41201362013816833"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试完成后，一些可视化信息已经以 PDF 格式存储在 test_results 文件夹中。例如：\n",
    "\n",
    "![result ETTm1 2440](./result.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
