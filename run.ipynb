{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast\n",
    "from exp.exp_imputation import Exp_Imputation\n",
    "from exp.exp_short_term_forecasting import Exp_Short_Term_Forecast\n",
    "from exp.exp_anomaly_detection import Exp_Anomaly_Detection\n",
    "from exp.exp_classification import Exp_Classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from data_provider.data_creat import *\n",
    "import akshare as ak\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    '''基本配置'''\n",
    "    # 选项：[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]')\n",
    "    task_name = 'long_term_forecast'\n",
    "    is_training = 1\n",
    "    model_id = 'Stock_96_96'\n",
    "    # 模型名称，选项：[Autoformer, Transformer, TimesNet]\n",
    "    model = 'TimesNet'\n",
    "    \n",
    "    '''股票数据获取'''\n",
    "    fuquan = 'hfq'# 设置复权方式,adjust=空选择的不复权，qfq是前复权，应该用hfq后复权来进行量化分析\n",
    "    period = 'daily' # 拉取时间周期{'daily', 'weekly', 'monthly'}\n",
    "    start_date = '20151201'  # 20151201 下载数据的开始日期,0就是公司上市时间\n",
    "    end_date = '-1'  # 下载数据的结束日期,如果0则到最后一天,如果-1是昨天.\n",
    "    final_data_feat =  ['index', 'Volume','Tom_Chg'] # 删除不需要列的标签\n",
    "    label_n = 5 # 预测未来连续多少天的收益率\n",
    "    # 标签是否替换成1或者0\n",
    "    zhangfu = 0.01  # 预测涨幅大于等于3%的为1，小于3%的为0\n",
    "    label_ch = True  # 如果是True ，预测n天以后上涨大于变量zhangfu为1，小于为0\n",
    "    \n",
    "    \n",
    "    '''数据加载'''\n",
    "    # 数据集类型,选项：[ETTh1,ETTh2,ETTm1,ETTm2,custom,m4,PSM,MSL,SMAP,SMD,SWAT,UEA]\n",
    "    data = 'custom'\n",
    "    root_path = './dataset/Stock/'\n",
    "    data_path = 'Stock.csv'\n",
    "    # 数据拼接前进行数据归一化\n",
    "    data_scaler = False\n",
    "    # 数据拼接不同股票中间添加间隔\n",
    "    data_addzero = True\n",
    "    # 预测任务 M:多变量预测多变量, S:单变量预测单变量, MS:多变量预测单变量\n",
    "    features = 'MS'\n",
    "    # 目标列名，S或MS任务中的目标特征\n",
    "    target = 'OT'\n",
    "    # 时间采集粒度，选项：[s:秒, t:分钟, h:小时, d:天, b:工作日, w:周, m:月]\n",
    "    freq = 'd'\n",
    "    # 模型检查点的位置\n",
    "    checkpoints = './checkpoints/'\n",
    "\n",
    "    '''预测任务'''\n",
    "    # 输入序列长度,这是用于模型训练的输入序列的长度\n",
    "    seq_len = 60\n",
    "    # 开始标记长度,这是模型输出目标中有标签数据的长度，类似于滑动窗口的长度\n",
    "    label_len = 20\n",
    "    # 预测序列长度\n",
    "    pred_len = 1\n",
    "    # 季节模式（针对M4数据集）\n",
    "    seasonal_patterns = 'Monthly'\n",
    "    inverse = False    # 反转输出数据\n",
    "\n",
    "    '''插补任务'''\n",
    "    # 插补任务中数据丢失率\n",
    "    mask_rate = 0.25\n",
    "\n",
    "    '''异常检测任务'''\n",
    "    # 异常检测中异常点占比\n",
    "    anomaly_ratio = 0.25\n",
    "\n",
    "    '''模型定义'''\n",
    "    # TimesBlock 中傅里叶变换,频率排名前k个周期\n",
    "    top_k = 5\n",
    "    # Inception 中卷积核个数\n",
    "    num_kernels = 6\n",
    "    # encoder 输入特征数\n",
    "    enc_in = 38\n",
    "    # decoder 输入特征数\n",
    "    dec_in = 38\n",
    "    # 输出通道数\n",
    "    c_out = 38\n",
    "    # 线性层隐含神经元个数\n",
    "    d_model = 32\n",
    "    # FFN 层隐含神经元个数\n",
    "    d_ff = 32\n",
    "    # 多头注意力机制\n",
    "    n_heads = 8\n",
    "    # encoder 层数\n",
    "    e_layers = 2\n",
    "    # decoder 层数\n",
    "    d_layers = 1\n",
    "    # 滑动窗口长度\n",
    "    moving_avg = 20\n",
    "    # 对 Q 进行采样，对 Q 采样的因子数\n",
    "    factor = 3\n",
    "    # 是否下采样操作 pooling\n",
    "    distil = True\n",
    "    # dropout 率\n",
    "    dropout = 0.1\n",
    "    # 时间特征嵌入方式,选项：[timeF, fixed, learned]\n",
    "    embed = 'timeF'\n",
    "    # 激活函数类型\n",
    "    activation = 'gelu'\n",
    "    # 是否输出 attention\n",
    "    output_attention = False\n",
    "\n",
    "    '''优化'''\n",
    "    # 并行核心数\n",
    "    num_workers = 10\n",
    "    # 实验轮数\n",
    "    itr = 1\n",
    "    # 训练迭代次数\n",
    "    train_epochs = 500\n",
    "    # batch size 大小\n",
    "    batch_size = 60\n",
    "    # early stopping 机制容忍次数\n",
    "    patience = 5\n",
    "    # 学习率\n",
    "    learning_rate = 0.0001\n",
    "    # 实验描述\n",
    "    des = 'stock'\n",
    "    # 损失函数\n",
    "    loss = 'MSE'\n",
    "    # 学习率下降策略\n",
    "    lradj = 'type1'\n",
    "    # 使用混合精度训练\n",
    "    use_amp = False\n",
    "\n",
    "    '''GPU'''\n",
    "    # 使用 gpu\n",
    "    use_gpu = True\n",
    "    gpu = 0\n",
    "    # 使用多个 gpus\n",
    "    use_multi_gpu = False\n",
    "    # 多 gpu 的设备 id\n",
    "    devices = '0,1,2,3'\n",
    "\n",
    "    '''去平稳化投影仪参数'''\n",
    "    # 投影仪的隐藏层维度（列表）\n",
    "    p_hidden_dims = [128, 128]\n",
    "    # 投影仪中的隐藏层数\n",
    "    p_hidden_layers = 2\n",
    "\n",
    "\n",
    "# 创建参数对象\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_down = ak.stock_cy_a_spot_em() # 创业板实时数据\n",
    "stock_list = stock_down[~stock_down['名称'].str.contains(\"退|ST\") & (stock_down['流通市值'] <= 1e11) & (stock_down['总市值'] >= 45e8)] # 去除退市和ST股票\n",
    "# 保存数据，编码格式为utf-8\n",
    "file_name = 'Stock_list.csv'\n",
    "stock_list.to_csv(args.root_path + file_name,index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (493, 9)\n",
      "添加数据以后形状： (493, 17)\n",
      "添加label以后数据形状: (493, 19)\n",
      "删除指定行、列后数据形状:  (463, 17)\n",
      "处理进度: 1/608 (0.16%)\n",
      "当前all_data的形状: (483, 17)\n",
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (1796, 9)\n",
      "添加数据以后形状： (1796, 17)\n",
      "添加label以后数据形状: (1796, 19)\n",
      "删除指定行、列后数据形状:  (1766, 17)\n",
      "处理进度: 2/608 (0.33%)\n",
      "当前all_data的形状: (2269, 17)\n",
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (1924, 9)\n",
      "添加数据以后形状： (1924, 17)\n",
      "添加label以后数据形状: (1924, 19)\n",
      "删除指定行、列后数据形状:  (1894, 17)\n",
      "处理进度: 3/608 (0.49%)\n",
      "当前all_data的形状: (4183, 17)\n",
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (1183, 9)\n",
      "添加数据以后形状： (1183, 17)\n",
      "添加label以后数据形状: (1183, 19)\n",
      "删除指定行、列后数据形状:  (1153, 17)\n",
      "处理进度: 4/608 (0.66%)\n",
      "当前all_data的形状: (5356, 17)\n",
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (1158, 9)\n",
      "添加数据以后形状： (1158, 17)\n",
      "添加label以后数据形状: (1158, 19)\n",
      "删除指定行、列后数据形状:  (1128, 17)\n",
      "处理进度: 5/608 (0.82%)\n",
      "当前all_data的形状: (6504, 17)\n",
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (1956, 9)\n",
      "添加数据以后形状： (1956, 17)\n",
      "添加label以后数据形状: (1956, 19)\n",
      "删除指定行、列后数据形状:  (1926, 17)\n",
      "处理进度: 6/608 (0.99%)\n",
      "当前all_data的形状: (8450, 17)\n",
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (1875, 9)\n",
      "添加数据以后形状： (1875, 17)\n",
      "添加label以后数据形状: (1875, 19)\n",
      "删除指定行、列后数据形状:  (1845, 17)\n",
      "处理进度: 7/608 (1.15%)\n",
      "当前all_data的形状: (10315, 17)\n",
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (1951, 9)\n",
      "添加数据以后形状： (1951, 17)\n",
      "添加label以后数据形状: (1951, 19)\n",
      "删除指定行、列后数据形状:  (1921, 17)\n",
      "处理进度: 8/608 (1.32%)\n",
      "当前all_data的形状: (12256, 17)\n",
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (54, 9)\n",
      "股票代码 301558 的数据长度小于300,跳过此次循环。\n",
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (1105, 9)\n",
      "添加数据以后形状： (1105, 17)\n",
      "添加label以后数据形状: (1105, 19)\n",
      "删除指定行、列后数据形状:  (1075, 17)\n",
      "处理进度: 10/608 (1.64%)\n",
      "当前all_data的形状: (13351, 17)\n",
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (1939, 9)\n",
      "添加数据以后形状： (1939, 17)\n",
      "添加label以后数据形状: (1939, 19)\n",
      "删除指定行、列后数据形状:  (1909, 17)\n",
      "处理进度: 11/608 (1.81%)\n",
      "当前all_data的形状: (15280, 17)\n",
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (717, 9)\n",
      "添加数据以后形状： (717, 17)\n",
      "添加label以后数据形状: (717, 19)\n",
      "删除指定行、列后数据形状:  (687, 17)\n",
      "处理进度: 12/608 (1.97%)\n",
      "当前all_data的形状: (15987, 17)\n",
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (1534, 9)\n",
      "添加数据以后形状： (1534, 17)\n",
      "添加label以后数据形状: (1534, 19)\n",
      "删除指定行、列后数据形状:  (1504, 17)\n",
      "处理进度: 13/608 (2.14%)\n",
      "当前all_data的形状: (17511, 17)\n",
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (843, 9)\n",
      "添加数据以后形状： (843, 17)\n",
      "添加label以后数据形状: (843, 19)\n",
      "删除指定行、列后数据形状:  (813, 17)\n",
      "处理进度: 14/608 (2.30%)\n",
      "当前all_data的形状: (18344, 17)\n",
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (1961, 9)\n"
     ]
    }
   ],
   "source": [
    "all_data_raw = pd.DataFrame()  # 初始化一个空的 DataFrame 用于存储原始数据\n",
    "all_data_scaled = pd.DataFrame()  # 初始化一个空的 DataFrame 用于存储标准化后的数据\n",
    "processed_count = 0  # 初始化计数器\n",
    "total_count = len(stock_list[\"代码\"].values)  # 获取总股票数量\n",
    "\n",
    "for i in stock_list[\"代码\"].values:\n",
    "    NUM = i\n",
    "    try:\n",
    "        # 下载原始数据\n",
    "        raw_data = download_data(NUM, args)\n",
    "        # 更新已处理股票数量计数器\n",
    "        processed_count += 1\n",
    "        # 检查数据长度，如果小于300则跳过此次循环\n",
    "        if raw_data.shape[0] < 300:\n",
    "            print(f\"股票代码 {NUM} 的数据长度小于300,跳过此次循环。\")\n",
    "            continue\n",
    "        \n",
    "        # 拼接数据，添加各种参数\n",
    "        ad_data = add_data(raw_data.copy(), args)\n",
    "        # 添加预测标签\n",
    "        ot_data = add_label(ad_data.copy(), args)\n",
    "        # 删除无效数据\n",
    "        su_data = sub_data(ot_data.copy(), args)\n",
    "\n",
    "        # 存储未经标准化的数据\n",
    "        all_data_raw = pd.concat([all_data_raw, su_data], ignore_index=True)\n",
    "\n",
    "        # 标准化处理\n",
    "        scaler = StandardScaler()\n",
    "        non_time_columns = su_data.columns[1:-1]  # 假设时间列是第一列\n",
    "        su_data_scaled = su_data.copy()\n",
    "        su_data_scaled[non_time_columns] = scaler.fit_transform(su_data[non_time_columns])\n",
    "\n",
    "        # 存储经过标准化的数据\n",
    "        all_data_scaled = pd.concat([all_data_scaled, su_data_scaled], ignore_index=True)\n",
    "\n",
    "        # 如果args.data_addzero为True，并且all_data不是空的，先插入20行全为0的数据\n",
    "        if args.data_addzero and not all_data_raw.empty:\n",
    "            zero_data_raw = pd.DataFrame(0, index=range(20), columns=all_data_raw.columns)\n",
    "            all_data_raw = pd.concat([all_data_raw, zero_data_raw], ignore_index=True)\n",
    "\n",
    "        # 如果args.data_addzero为True，并且all_data_scaled不是空的，先插入20行全为0的数据\n",
    "        if args.data_addzero and not all_data_scaled.empty:\n",
    "            zero_data_scaled = pd.DataFrame(0, index=range(20), columns=all_data_scaled.columns)\n",
    "            all_data_scaled = pd.concat([all_data_scaled, zero_data_scaled], ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"处理股票代码 {NUM} 时出现错误: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 计算并打印处理进度\n",
    "    progress = processed_count / total_count\n",
    "    print(f\"处理进度: {processed_count}/{total_count} ({progress:.2%})\")\n",
    "    print(f\"当前all_data的形状: {all_data_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存原始数据到 CSV 文件\n",
    "file_name_raw = \"all_data_raw.csv\"\n",
    "all_data_raw.to_csv(args.root_path + file_name_raw, index=False)\n",
    "\n",
    "# 保存标准化后的数据到 CSV 文件\n",
    "file_name_scaled = \"all_data_scaled.csv\"\n",
    "all_data_scaled.to_csv(args.root_path + file_name_scaled, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子以确保结果可重现\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "\n",
    "# 获取列数\n",
    "num_columns = all_data.shape[1]\n",
    "# args.des = NUM\n",
    "args.enc_in = num_columns - 1\n",
    "args.dec_in = num_columns - 1\n",
    "args.c_out = num_columns - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 CPU.\n",
      "Use CPU\n",
      ">>>>>>>开始训练 : long_term_forecast_Stock_96_96_TimesNet_custom_ftMS_sl60_ll20_pl1_dm32_nh8_el2_dl1_df32_fc3_ebtimeF_dtTrue_stock_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1291\n",
      "val 193\n",
      "test 386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 38, 3], expected input[60, 16, 62] to have 38 channels, but got 16 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m exp \u001b[38;5;241m=\u001b[39m Exp(args)  \u001b[38;5;66;03m# 设置实验\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>开始训练 : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n\u001b[1;32m---> 64\u001b[0m exp\u001b[38;5;241m.\u001b[39mtrain(setting)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>测试 : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n\u001b[0;32m     67\u001b[0m exp\u001b[38;5;241m.\u001b[39mtest(setting)\n",
      "File \u001b[1;32md:\\pytorch\\Time-Series-Library\\exp\\exp_long_term_forecasting.py:150\u001b[0m, in \u001b[0;36mExp_Long_Term_Forecast.train\u001b[1;34m(self, setting)\u001b[0m\n\u001b[0;32m    148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_x, batch_x_mark, dec_inp, batch_y_mark)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# 如果预测方式为MS，取最后1列否则取第1列\u001b[39;00m\n\u001b[0;32m    153\u001b[0m f_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\my\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\pytorch\\Time-Series-Library\\models\\TimesNet.py:244\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc, x_mark_enc, x_dec, x_mark_dec, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong_term_forecast\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshort_term_forecast\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m         dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dec_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :]  \u001b[38;5;66;03m# [B, L, D]\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputation\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32md:\\pytorch\\Time-Series-Library\\models\\TimesNet.py:148\u001b[0m, in \u001b[0;36mModel.forecast\u001b[1;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[0;32m    145\u001b[0m x_enc \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m stdev\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# embedding\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_embedding(x_enc, x_mark_enc)  \u001b[38;5;66;03m# [B,T,C]\u001b[39;00m\n\u001b[0;32m    149\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_linear(enc_out\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# align temporal dimension\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# TimesNet\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\my\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\pytorch\\Time-Series-Library\\layers\\Embed.py:126\u001b[0m, in \u001b[0;36mDataEmbedding.forward\u001b[1;34m(self, x, x_mark)\u001b[0m\n\u001b[0;32m    123\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_embedding(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(x)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m# 如果有时间标记，则使用数值嵌入、时间嵌入和位置嵌入\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_embedding(\n\u001b[0;32m    127\u001b[0m         x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_embedding(x_mark) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(x)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\my\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\pytorch\\Time-Series-Library\\layers\\Embed.py:41\u001b[0m, in \u001b[0;36mTokenEmbedding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenConv(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\my\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\my\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\my\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_conv_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, weight: Tensor, bias: Optional[Tensor]):\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 306\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    307\u001b[0m                         weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    308\u001b[0m                         _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    310\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 38, 3], expected input[60, 16, 62] to have 38 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "# 检查并设置 GPU\n",
    "args.use_gpu = torch.cuda.is_available() and args.use_gpu\n",
    "if args.use_gpu:\n",
    "    print(\"使用 GPU.\")\n",
    "    total_cuda_devices = torch.cuda.device_count()  # 获取系统中可用的 GPU 总数\n",
    "    print(f\"系统中总共有 {total_cuda_devices} 个 CUDA 设备可用。\")\n",
    "    if args.use_multi_gpu:\n",
    "        args.devices = args.devices.replace(' ', '')\n",
    "        device_ids = args.devices.split(',')\n",
    "        args.device_ids = [int(id_) for id_ in device_ids]\n",
    "        args.gpu = args.device_ids[0]\n",
    "        \n",
    "        # 打印多 GPU 使用情况\n",
    "        print(f\"使用多个GPU: {args.device_ids}\")\n",
    "        device = torch.device(f\"cuda:{args.gpu}\" if args.use_gpu else \"cpu\")\n",
    "        print(f\"Primary GPU (cuda:{args.gpu}) is in use.\")\n",
    "    else:\n",
    "        args.gpu = 0\n",
    "        device = torch.device(\"cuda\" if args.use_gpu else \"cpu\")\n",
    "        print(\"使用单个 GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"使用 CPU.\")\n",
    "\n",
    "# 选择合适的实验类\n",
    "if args.task_name == 'long_term_forecast':\n",
    "    Exp = Exp_Long_Term_Forecast\n",
    "elif args.task_name == 'short_term_forecast':\n",
    "    Exp = Exp_Short_Term_Forecast\n",
    "elif args.task_name == 'imputation':\n",
    "    Exp = Exp_Imputation\n",
    "elif args.task_name == 'anomaly_detection':\n",
    "    Exp = Exp_Anomaly_Detection\n",
    "elif args.task_name == 'classification':\n",
    "    Exp = Exp_Classification\n",
    "else:\n",
    "    Exp = Exp_Long_Term_Forecast  # 默认情况\n",
    "\n",
    "# 进行训练和测试\n",
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # 实验记录设置\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.task_name,\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # 设置实验\n",
    "        print('>>>>>>>开始训练 : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>测试 : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "        if args.use_gpu:\n",
    "            torch.cuda.empty_cache()\n",
    "else:\n",
    "    ii = 0\n",
    "    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.distil,\n",
    "        args.des, ii)\n",
    "\n",
    "    exp = Exp(args)  # 设置实验\n",
    "    print('>>>>>>>测试 : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting, test=1)\n",
    "    if args.use_gpu:\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=./runs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
