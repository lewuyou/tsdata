{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast\n",
    "from exp.exp_imputation import Exp_Imputation\n",
    "from exp.exp_short_term_forecasting import Exp_Short_Term_Forecast\n",
    "from exp.exp_anomaly_detection import Exp_Anomaly_Detection\n",
    "from exp.exp_classification import Exp_Classification\n",
    "from data_provider.data_creat import *\n",
    "import akshare as ak\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    '''基本配置'''\n",
    "    # 选项：[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]')\n",
    "    task_name = 'long_term_forecast'\n",
    "    is_training = 1\n",
    "    model_id = 'Stock_96_96'\n",
    "    # 模型名称，选项：[Autoformer, Transformer, TimesNet]\n",
    "    model = 'TimesNet'\n",
    "    \n",
    "    '''股票数据获取'''\n",
    "    fuquan = 'hfq'# 设置复权方式,adjust=空选择的不复权，qfq是前复权，应该用hfq后复权来进行量化分析\n",
    "    period = 'daily' # 拉取时间周期{'daily', 'weekly', 'monthly'}\n",
    "    start_date = '20161201'  # 下载数据的开始日期,0就是公司上市时间\n",
    "    end_date = '-1'  # 下载数据的结束日期,如果0则到最后一天,如果-1是昨天.\n",
    "    final_data_feat =  ['Open', 'Close', 'High', 'Low', 'K_valume','Amplitude', 'Change', 'Turnover', 'macd', 'rsi',\\\n",
    "        'kdj_k', 'kdj_d', 'twf_feat', 'lable']\n",
    "    # 选择拼接前需要归一化的特征\n",
    "    feat_nor_lise = ['Open', 'Close', 'High', 'Low', 'K_valume','Amplitude', 'Change', 'Turnover', 'macd', 'rsi', 'kdj_k', 'kdj_d', 'twf_feat',]\n",
    "    label_n = 7 # 预测未来连续多少天的收益率\n",
    "    zhangfu = 0.03  # 预测涨幅大于等于3%的为1，小于3%的为0\n",
    "    label_ch = False  # 如果是True ，预测n天以后上涨大于变量zhangfu为1，小于为0\n",
    "    \n",
    "    \n",
    "    '''数据加载'''\n",
    "    # 数据集类型,选项：[ETTh1,ETTh2,ETTm1,ETTm2,custom,m4,PSM,MSL,SMAP,SMD,SWAT,UEA]\n",
    "    data = 'custom'\n",
    "    root_path = './dataset/Stock/'\n",
    "    data_path = 'Stock.csv'\n",
    "    # 预测任务 M:多变量预测多变量, S:单变量预测单变量, MS:多变量预测单变量\n",
    "    features = 'MS'\n",
    "    # 目标列名，S或MS任务中的目标特征\n",
    "    target = 'OT'\n",
    "    # 时间采集粒度，选项：[s:秒, t:分钟, h:小时, d:天, b:工作日, w:周, m:月]\n",
    "    freq = 'd'\n",
    "    # 模型检查点的位置\n",
    "    checkpoints = './checkpoints/'\n",
    "\n",
    "    '''预测任务'''\n",
    "    # 输入序列长度\n",
    "    seq_len = 96\n",
    "    # 开始标记长度\n",
    "    label_len = 48\n",
    "    # 预测序列长度\n",
    "    pred_len = 1\n",
    "    # 季节模式（针对M4数据集）\n",
    "    seasonal_patterns = 'Monthly'\n",
    "    inverse = False    # 反转输出数据\n",
    "\n",
    "    '''插补任务'''\n",
    "    # 插补任务中数据丢失率\n",
    "    mask_rate = 0.25\n",
    "\n",
    "    '''异常检测任务'''\n",
    "    # 异常检测中异常点占比\n",
    "    anomaly_ratio = 0.25\n",
    "\n",
    "    '''模型定义'''\n",
    "    # TimesBlock 中傅里叶变换,频率排名前k个周期\n",
    "    top_k = 5\n",
    "    # Inception 中卷积核个数\n",
    "    num_kernels = 6\n",
    "    # encoder 输入特征数\n",
    "    enc_in = 21\n",
    "    # decoder 输入特征数\n",
    "    dec_in = 21\n",
    "    # 输出通道数\n",
    "    c_out = 21\n",
    "    # 线性层隐含神经元个数\n",
    "    d_model = 32\n",
    "    # FFN 层隐含神经元个数\n",
    "    d_ff = 32\n",
    "    # 多头注意力机制\n",
    "    n_heads = 8\n",
    "    # encoder 层数\n",
    "    e_layers = 2\n",
    "    # decoder 层数\n",
    "    d_layers = 1\n",
    "    # 滑动窗口长度\n",
    "    moving_avg = 25\n",
    "    # 对 Q 进行采样，对 Q 采样的因子数\n",
    "    factor = 3\n",
    "    # 是否下采样操作 pooling\n",
    "    distil = True\n",
    "    # dropout 率\n",
    "    dropout = 0.1\n",
    "    # 时间特征嵌入方式,选项：[timeF, fixed, learned]\n",
    "    embed = 'timeF'\n",
    "    # 激活函数类型\n",
    "    activation = 'gelu'\n",
    "    # 是否输出 attention\n",
    "    output_attention = False\n",
    "\n",
    "    '''优化'''\n",
    "    # 并行核心数\n",
    "    num_workers = 10\n",
    "    # 实验轮数\n",
    "    itr = 1\n",
    "    # 训练迭代次数\n",
    "    train_epochs = 10\n",
    "    # batch size 大小\n",
    "    batch_size = 32\n",
    "    # early stopping 机制容忍次数\n",
    "    patience = 3\n",
    "    # 学习率\n",
    "    learning_rate = 0.0001\n",
    "    # 实验描述\n",
    "    des = 'test'\n",
    "    # 损失函数\n",
    "    loss = 'MSE'\n",
    "    # 学习率下降策略\n",
    "    lradj = 'type1'\n",
    "    # 使用混合精度训练\n",
    "    use_amp = False\n",
    "\n",
    "    '''GPU'''\n",
    "    # 使用 gpu\n",
    "    use_gpu = False\n",
    "    gpu = 0\n",
    "    # 使用多个 gpus\n",
    "    use_multi_gpu = False\n",
    "    # 多 gpu 的设备 id\n",
    "    devices = '0,1,2,3'\n",
    "\n",
    "    '''去平稳化投影仪参数'''\n",
    "    # 投影仪的隐藏层维度（列表）\n",
    "    p_hidden_dims = [128, 128]\n",
    "    # 投影仪中的隐藏层数\n",
    "    p_hidden_layers = 2\n",
    "\n",
    "\n",
    "# 创建参数对象\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyti.moving_average_convergence_divergence import moving_average_convergence_divergence as macd\n",
    "from pyti.relative_strength_index import  relative_strength_index as rsi\n",
    "from pyti.stochastic import percent_k as kdj_k\n",
    "from pyti.stochastic import percent_d as kdj_d\n",
    "from data_provider.func_util import *\n",
    "from data_provider.func_stock import *\n",
    "from data_provider.func_RankGauss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data(raw_data,args):\n",
    "    '''创建模型合适的数据集train_loader, valid_loader, test_loader'''\n",
    "\n",
    "    # 提取列数据为NumPy数组\n",
    "    open_data = raw_data['Open'].values\n",
    "    close_data = raw_data['Close'].values\n",
    "    high_data = raw_data['High'].values\n",
    "    low_data = raw_data['Low'].values\n",
    "    volume_data = raw_data['Volume'].values\n",
    "\n",
    "    # 指标计算\n",
    "    raw_data['macd'] = macd(close_data, 12, 26)\n",
    "    raw_data['rsi'] = rsi(close_data, 14)\n",
    "    raw_data['kdj_k'] = kdj_k(close_data, 9)\n",
    "    raw_data['kdj_d'] = kdj_d(close_data, 9)\n",
    "    raw_data['twf_feat'] = twf_feat(close_data, high_data, low_data, volume_data, 21)\n",
    "    raw_data['K_volume'] = volume_data / 1000  # 注意列名修正为 'K_volume'\n",
    "    print(f'添加数据以后形状： {raw_data.shape}')\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_data(raw_data,args):\n",
    "    '''\n",
    "    添加预测目标lable列,并将所有的nan值填充为0'''\n",
    "    lable_data = add_lable(raw_data,args.zhangfu,args.lable_n, args.lable_ch)\n",
    "    '''\n",
    "    删除第一列date列,删除前26行因为macd和cmf是空值,\n",
    "    删除最后config['feat_n']行因为未来lable的n天的数据是空值,影响训练'''\n",
    "    lable_data = lable_data.iloc[26:-args.lable_n, 1:]\n",
    "    '''\n",
    "    手动选择拼接前原始数据列'''\n",
    "    final_data = lable_data[args.final_data_feat].copy() \n",
    "    print('拼接前data形状: ',final_data.shape)\n",
    "    print('拼接前数据有: ',args.final_data_feat)\n",
    "    '''\n",
    "    如果标签为True,那么就对final_data的change,twf列进行归一化'''\n",
    "    if args.feat_normalize == 1:\n",
    "        for i in args.feat_nor_lise:\n",
    "            final_data[i] = normalize(final_data[i])\n",
    "        print('拼接前对数据进行归一化处理')\n",
    "    elif args.feat_normalize == 2:\n",
    "        for i in args.feat_nor_lise:\n",
    "            final_data[i] = Standardization(final_data[i])\n",
    "        print('拼接前对数据进行标准化处理')\n",
    "    elif args.feat_normalize == 3:\n",
    "        for i in args.feat_nor_lise:\n",
    "            final_data[i] = QTF(final_data[i])\n",
    "        print('拼接前对数据进行均匀分布转换QuantileTransformer处理')\n",
    "    elif args.feat_normalize == 4:\n",
    "        for i in args.feat_nor_lise:\n",
    "            rg_data = final_data[i].values\n",
    "            Rank = RankGauss_norm()\n",
    "            final_data[i] = Rank.fit_transform(rg_data)\n",
    "        print('拼接前对数据进行高斯标准化Rank Gaussian Normalization处理')\n",
    "    elif args.feat_normalize == 5:\n",
    "        for i in args.feat_nor_lise:\n",
    "            final_data[i] = PowerTransformer(final_data[i])\n",
    "        print('拼接前对数据进行高斯分布转换PowerTransformer处理')\n",
    "    elif args.feat_normalize == 6:\n",
    "        for i in args.feat_nor_lise:\n",
    "            final_data[i] = RobustScaler(final_data[i])\n",
    "        print('拼接前对数据进行鲁棒缩放RobustScaler处理')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(data, args):\n",
    "    '''原始数据计算并添加预测标签'''\n",
    "    data['Tom_Chg'] = (data['Close'].shift(-args.label_n) - data['Close'])/ data['Close']# 计算第n天到今天收益率\n",
    "    if args.label_ch:\n",
    "        # 初始化OT列为0\n",
    "        data['OT'] = 0\n",
    "        # 如果第n天到今天收益率data['Tom_Chg']大于0.5，那么label就等于1\n",
    "        data.loc[data['Tom_Chg'] >= args.zhangfu, 'OT'] = 1\n",
    "    else:\n",
    "        data['OT'] = data['Tom_Chg']\n",
    "    data = data.fillna(0)#将数据中的nan替换为0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取数据时间为： 20161201 - 20231214\n",
      "数据形状： (1712, 9)\n",
      "添加数据以后形状： (1712, 15)\n"
     ]
    }
   ],
   "source": [
    "stock_list = ['600028'] # 测试用\n",
    "for i in stock_list:\n",
    "    NUM = i\n",
    "    # 获取数据\n",
    "    raw_data = download_data(NUM, args)\n",
    "    # 拼接数据\n",
    "    a_data = add_data(raw_data.copy(), args)\n",
    "    # 添加标签\n",
    "    s_data = add_label(a_data.copy(), args)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子以确保结果可重现\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n",
      ">>>>>>>开始训练 : long_term_forecast_Stock_96_96_TimesNet_custom_ftMS_sl96_ll48_pl1_dm32_nh8_el2_dl1_df32_fc3_ebtimeF_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 36791\n",
      "val 5270\n",
      "test 10539\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m exp \u001b[38;5;241m=\u001b[39m Exp(args)  \u001b[38;5;66;03m# 设置实验\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>开始训练 : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n\u001b[1;32m---> 48\u001b[0m exp\u001b[38;5;241m.\u001b[39mtrain(setting)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>测试 : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n\u001b[0;32m     51\u001b[0m exp\u001b[38;5;241m.\u001b[39mtest(setting)\n",
      "File \u001b[1;32md:\\pytorch\\Time-Series-Library\\exp\\exp_long_term_forecasting.py:145\u001b[0m, in \u001b[0;36mExp_Long_Term_Forecast.train\u001b[1;34m(self, setting)\u001b[0m\n\u001b[0;32m    143\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_x, batch_x_mark, dec_inp, batch_y_mark)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# 如果预测方式为MS，取最后1列否则取第1列\u001b[39;00m\n\u001b[0;32m    148\u001b[0m f_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\my\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\pytorch\\Time-Series-Library\\models\\TimesNet.py:244\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc, x_mark_enc, x_dec, x_mark_dec, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong_term_forecast\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshort_term_forecast\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m         dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dec_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :]  \u001b[38;5;66;03m# [B, L, D]\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputation\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32md:\\pytorch\\Time-Series-Library\\models\\TimesNet.py:153\u001b[0m, in \u001b[0;36mModel.forecast\u001b[1;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# TimesNet\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer):\n\u001b[1;32m--> 153\u001b[0m     enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel[i](enc_out))\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# porject back  映射层\u001b[39;00m\n\u001b[0;32m    155\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(enc_out)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\my\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\pytorch\\Time-Series-Library\\models\\TimesNet.py:70\u001b[0m, in \u001b[0;36mTimesBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     66\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mreshape(B, length \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m period, period,\n\u001b[0;32m     67\u001b[0m                   N)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# 2D conv: from 1d Variation to 2d Variation\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# 进入卷积网络\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(out)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# reshape back\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# 重塑结果(batch, -1, feature)\u001b[39;00m\n\u001b[0;32m     73\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, N)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\my\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\my\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\my\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\pytorch\\Time-Series-Library\\layers\\Conv_Blocks.py:35\u001b[0m, in \u001b[0;36mInception_Block_V1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m     res_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernels[i](x))\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 将列表沿最后一个维度拼接, 然后在最后一个维度求平均\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(res_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 检查并设置 GPU\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ', '')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "\n",
    "# 选择合适的实验类\n",
    "if args.task_name == 'long_term_forecast':\n",
    "    Exp = Exp_Long_Term_Forecast\n",
    "elif args.task_name == 'short_term_forecast':\n",
    "    Exp = Exp_Short_Term_Forecast\n",
    "elif args.task_name == 'imputation':\n",
    "    Exp = Exp_Imputation\n",
    "elif args.task_name == 'anomaly_detection':\n",
    "    Exp = Exp_Anomaly_Detection\n",
    "elif args.task_name == 'classification':\n",
    "    Exp = Exp_Classification\n",
    "else:\n",
    "    Exp = Exp_Long_Term_Forecast  # 默认情况\n",
    "\n",
    "# 进行训练和测试\n",
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # 实验记录设置\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.task_name,\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # 设置实验\n",
    "        print('>>>>>>>开始训练 : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>测试 : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "        if args.use_gpu:\n",
    "            torch.cuda.empty_cache()\n",
    "else:\n",
    "    ii = 0\n",
    "    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.distil,\n",
    "        args.des, ii)\n",
    "\n",
    "    exp = Exp(args)  # 设置实验\n",
    "    print('>>>>>>>测试 : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting, test=1)\n",
    "    if args.use_gpu:\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
